---
title: "Group Project 1 | STAT 330"

author: <center> Elizabeth Mohler, Peter Welty, Jeong Kim <center>
output: html_document
---

<style type="text/css">
h1.title {
  font-size: 40px;
  text-align: center;
}
</style>

```{r setup, include=FALSE}
# load any necessary packages here
library(tidyverse)
library(ggfortify)
library(car)
sz <- 16
```


## Data and Description

We are interested to see if there is a correlation between crime rates and average rent in a city in the United States. We hypothesize that rent with have a significant affect on crime rate, specifically that higher rent will increase crime. Our predictor variable is crime rate and how it might explain rent prices in a city.

To test our assumption, we got our crime data from the FBI's database from 2010-2015 and got our rent data for the same years from Zillow. We will begin our analysis by applying basic summary statistics and exploratory data techniques to better understand the data. Then, we apply simple linear regression.

The data sources are as follows:
Crime Dataset: https://www.kaggle.com/marshallproject/crime-rates/notebooks
Rent Dataset: https://www.kaggle.com/zillow/rent-index

To merge the datasets, we averaged the rent for the year on the rent dataset, tidyed both datasets, and joined using year, city, and state as the keys. 

We conclude our analysis by using what we learned to infer to ... We recommend ...

## Steps/Process

To analyze the data we will follow these basic steps outlined below:
1. Summarize data and prepare it for analysis
2. Check assumptions for a linear model
3. Transform data to better fit assumptions
4. Repeat 3 & 4 until transformation is sufficient
5. Analyze the relationship of crime rate and rent through confidence intervals, hypotheses tests, and find the success of our model. 

### Import and preprocessing data

The following table displays the variable names in this data set, along with their descriptions.

Variable   | Description
---------- | -------------
Crime Rate | The number of violent crimes (homicides, rapes, burglary, etc.) divided by the population
Rent       | The average rent price per year

```{r}
data <- read.csv(file = "Crime_vs_Rent_Group_Project.csv", header = TRUE)
```

#### Preprocessing
- Originally, there are two datasets. One for rent and one for crime. We merged them. 
- Remove NaNs in Avg_rent and crime_rate columns
```{r}
data <- data[!is.na(data$Avg_rent), ]
data <- data[!is.na(data$crime_rate),]
```


We start by applying basic summary and exploratory statistics to this data to better understand the data and identify trends.

```{r}
summary(data)
head(data)
```

```{r}
glimpse(data)
```

#### Correlation

```{r}
cor(data$Avg_rent, data$crime_rate)
```

```{r}
data.lm <- lm(formula = crime_rate ~ Avg_rent, data = data)
summary(data.lm)

data$residuals <- data.lm$residuals
data$fitted.values <- data.lm$fitted.values
```

### Basic Scatterplot
```{r}
data.base.plot <- ggplot(data = data, mapping = aes(x = Avg_rent, y = crime_rate)) + 
  geom_point() +
  ggtitle("Title") +
  xlab("Average Rent ($)") +
  ylab("Crime Rate") +
  scale_x_continuous(breaks = seq(0, 4000, by = 500),
                     minor_breaks = seq(0, 4000, by = 500)) +
  scale_y_continuous(breaks = seq(0, 0.025, by = 0.0025),
                     minor_breaks = seq(0, 0.025, by = 0.0025)) +
  theme(aspect.ratio = 1, plot.title = element_text(hjust = 0.5))
data.base.plot
```

####The OLS regression line along with the scatterplot

```{r}
data.base.plot + geom_smooth(method = "lm", se = FALSE) 
```

#### Residuals vs. Fitted Values Plot

```{r}
autoplot(data.lm, which = 1, ncol = 1, nrow = 1) +
  theme(aspect.ratio = 1, plot.title = element_text(hjust = 0.5))
```

#### Residuals vs. Predictor Plot

```{r}
ggplot(data = data, mapping = aes(y = residuals, x = Avg_rent)) +
  geom_point() +
  theme(aspect.ratio = 1)
```



#### Normal Probability Plot

```{r}
autoplot(data.lm, which = 2, ncol = 1, nrow = 1) +
  theme(aspect.ratio = 1, plot.title = element_text(hjust = 0.5))
```

#### Histogram for residuals

```{r}
ggplot(data = data, mapping = aes(x = residuals)) + 
  geom_histogram(mapping = aes(y = ..density..), binwidth = 0.0015) +
  stat_function(fun = dnorm, color = "red", size = 2,
                args = list(mean = mean(data$residuals), 
                            sd = sd(data$residuals))) +
  theme(aspect.ratio = 1)
```

#### Boxplot for residuals

```{r}
ggplot(data = data, mapping = aes(y = residuals)) +
  geom_boxplot() +
  theme(aspect.ratio = 1)
```

#### Sequence Plot

```{r}
ggplot(data) +
  geom_line(mapping = aes(x = 1:dim(data)[1], y = residuals)) +
  theme_bw() + 
  scale_y_continuous(limits = c(-0.008, 0.015)) +
  scale_x_continuous(limits = c(0, 350)) +
  xlab("TODO") +
  theme(axis.title.x = element_text(size = sz),
        axis.title.y = element_text(size = sz),
        axis.text = element_text(size = sz),
        aspect.ratio = 1)
```

#### Shapiro-Wilk Test

```{r}
shapiro.test(data$residuals)
```

#### Cook's Distance

```{r}
data$cooksd <- cooks.distance(data.lm)
# Find n
data[data$cooksd >= 4 / length(data$cooksd), ]
```

#### Brown-Forsythe Test

```{r}
grp <- as.factor(c(rep("lower", floor(dim(data)[1] / 2)), 
                   rep("upper", ceiling(dim(data)[1] / 2))))
leveneTest(data[order(data$Avg_rent), "residuals"] ~ grp, center = median)
```

#### DFBETAS

```{r}
# calculate the DFBETAS
data.dfbetas <- as.data.frame(dfbetas(data.lm))
# add the observation numbers to dataframe for plotting purposes
data.dfbetas$obs <- 1:length(data$Avg_rent)

# plot the DFBETAS against the observation number
ggplot(data = data.dfbetas) + 
  geom_point(mapping = aes(x = obs, y = abs(Avg_rent))) +
  ylab("Absolute Value of DFBETAS for Average Rent") +
  xlab("Observation Number") +
  geom_hline(mapping = aes(yintercept = 2 / sqrt(length(obs))),
             color = "red", linetype = "dashed") +
  #scale_x_continuous(limits = c(0, 300)) +
  #scale_y_continuous(limits = c(0, 0.25)) +
  theme_bw() +
  theme(
    axis.title.x = element_text(size = sz - 5),
    axis.title.y = element_text(size = sz - 5),
    axis.text = element_text(size = sz - 5),
    aspect.ratio = 1,
    plot.title = element_blank()
  )

# print a list of potential influential points according to DFBETAS
rent.extreme.dfbetas <- data.dfbetas[abs(data.dfbetas$Avg_rent) > 
                                         2 / sqrt(length(data.dfbetas$obs)), ]
rent.extreme.dfbetas[order(rent.extreme.dfbetas$Avg_rent), ]
```

#### DFFITS

```{r}
# calculate the DFFITS
data.dffits <- data.frame("dffits" = dffits(data.lm))
# add the observation numbers to dataframe for plotting purposes
data.dffits$obs <- 1:length(data$crime_rate)

# plot the DFFITS against the observation number
ggplot(data = data.dffits) + 
  geom_point(mapping = aes(x = obs, y = abs(dffits))) +
  ylab("Absolute Value of DFFITS for Y") +
  xlab("Observation Number") +
  geom_hline(mapping = aes(yintercept = 2 * 
                             sqrt(length(data.lm$coefficients) / length(obs))),
             color = "red", linetype = "dashed") +
  theme_bw() +
  #scale_x_continuous(limits = c(0, 300)) +
  #scale_y_continuous(limits = c(0, 0.3)) +
  theme(
    axis.title.x = element_text(size = sz - 5),
    axis.title.y = element_text(size = sz - 5),
    axis.text = element_text(size = sz - 5),
    aspect.ratio = 1,
    plot.title = element_blank()
  )

# print a list of potential influential points according to DFFITS
data.dffits[abs(data.dffits$dffits) > 2 * 
             sqrt(length(data.lm$coefficients) / length(data.dffits$obs)), ]
```




### Assumption Checking for The Model

#### X vs Y is linear
X vs Y is not linear, since the scatterplot does not look roughly linear and does not look like a line can be easily fitted to the data. As well the Residuals vs Fitted values plot shows that the blue line is not roughly horizontal with heavy curvature. 

#### The residuals are independent

The residuals seem not to be independent since crimes the recorded are only observed or reported to the FBI. Crime in general is not random but premeditated. However, average rent is more likely to be skewed by heavy outliers in cities depending on the area. Median rent prices might be a better indicator.

#### The residuals are normally distributed and centered at zero

The residuals are not normally distributed. The box plot is pretty right skewed with the mean not centered at zero with the whiskers having very different lengths. As well the right skew is found in the density and residual histogram. Lastly the shapiro-wilk tests gives us a very small and significant p-value less than 0.05 therefore we reject the null hypothesis that the residuals are normally distrubuted.

#### The residuals have equal/constant variance across all values of X

The residuals do  have equal/constant variance across all values of average rent. The residuals vs fitted values plot seems to show equal variance of average rent prices with slightly more variance on the right side . However, the brown-forsythe test gives us a non-significant p-value of 0.8168 which is greater than 0.05. Therefore we fail to reject the null hypothesis that the redisuals have equal variance across all values of average rent prices. 

#### The model describes all observations (i.e., there are no influential points)

The model seems to describe all observations, since the boxplot shows only about 4 outliers. The histogram shows possible a couple outliers. The Q-Q plot shows the same. However, the cooks distance shows many observation greater than 4/335 but they are pretty close to this cut-off. There is only one that seems to be significantly far from the cutoff and very unlikely to be influential due to the amount of observations. 

#### Additional predictor variables are not required.

Additional predictor variables may actually be required. Since it seems rent prices do not predict crime rates very well. Other predictor variables may be supply vs demand of housing in a city or State. Another predictor could be numbers of years since last renovations of housing. As well unemployment rates in a city could possibly predict crime rates.

### A Preliminary Conclusion
 Simple linear regression is not appropriate for this data set since few assumptions are met to even justify it. Another big indicator is the presence of many other predictor variables that could influence crime rates. Therefore it is possible that even transforming the current model may not yield any compelling results. It seems that Multiple linear regression could be a better route to be able to predict crime-rates. It would be inappropriate to make statistical inference on this model. We will try to transform the data and see if it still reveals x 


```{r, fig.align='center'}
# We need to transform our data, so we run a Box Cox
bc <- boxCox(data$Avg_rent ~ data$crime_rate)

data$rent_transformed <- sin(data$Avg_rent)
data$crime_transformed <- 1/log(data$crime_rate)


```
### Choosing a Tranformation
We knew we needed to tranform y (crime rate) significantly because the residuals were all over the place. We performed a box cox and saw the data was improving, but we then took the log of the transformed data, and it helped out nicely. We also transformed x which helped with normality in the data set. 


```{r}
transformed_data.lm <- lm(formula = crime_transformed ~ rent_transformed, data = data)
summary(transformed_data.lm)

data$residuals_t <- transformed_data.lm$residuals
data$fitted.values_t <- transformed_data.lm$fitted.values
```

#### Basic Scatterplot
```{r}
data.transformed.plot <- ggplot(data = data, mapping = aes(x = rent_transformed, y = crime_rate)) + 
  geom_point() +
  ggtitle("Tranformed Data Scatterplot") +
  xlab("sin(Average Rent) ($)") +
  ylab("log(1 /Crime Rate ^4/3)") +
  scale_x_continuous(breaks = seq(0, 4000, by = 500),
                     minor_breaks = seq(0, 4000, by = 500)) +
  scale_y_continuous(breaks = seq(0, 0.025, by = 0.0025),
                     minor_breaks = seq(0, 0.025, by = 0.0025)) +
  theme(aspect.ratio = 1, plot.title = element_text(hjust = 0.5))
data.transformed.plot
```

#### The OLS regression line along with the scatterplot

```{r}
data.transformed.plot + geom_smooth(method = "lm", se = FALSE) 
```

#### Residuals vs. Fitted Values Plot

```{r}
autoplot(transformed_data.lm, which = 1, ncol = 1, nrow = 1) +
  theme(aspect.ratio = 1, plot.title = element_text(hjust = 0.5))
```

#### Residuals vs. Predictor Plot

```{r}
ggplot(data = data, mapping = aes(y = residuals_t, x = rent_transformed)) +
  geom_point() +
  theme(aspect.ratio = 1)
```



#### Normal Probability Plot

```{r}
autoplot(transformed_data.lm, which = 2, ncol = 1, nrow = 1) +
  theme(aspect.ratio = 1, plot.title = element_text(hjust = 0.5))
```

#### Histogram for residuals

```{r}
ggplot(data = data, mapping = aes(x = residuals_t)) + 
  geom_histogram(mapping = aes(y = ..density..), binwidth = .005) +
  stat_function(fun = dnorm, color = "red", size = 2,
                args = list(mean = mean(data$residuals_t), 
                            sd = sd(data$residuals_t))) +
  theme(aspect.ratio = 1)
```

#### Boxplot for residuals

```{r}
ggplot(data = data, mapping = aes(y = residuals_t)) +
  geom_boxplot() +
  theme(aspect.ratio = 1)
```

#### Sequence Plot

```{r}
ggplot(data) +
  geom_line(mapping = aes(x = 1:dim(data)[1], y = residuals_t)) +
  theme_bw() + 
  scale_y_continuous(limits = c(-.5,.5)) +
  scale_x_continuous(limits = c(0,340)) +
  xlab("TODO") +
  theme(axis.title.x = element_text(size = sz),
        axis.title.y = element_text(size = sz),
        axis.text = element_text(size = sz),
        aspect.ratio = 1)
```

#### Shapiro-Wilk Test

```{r}
shapiro.test(data$residuals_t)
```

#### Cook's Distance

```{r}
data$cooksd_t <- cooks.distance(transformed_data.lm)
# Find n
data[data$cooksd_t >= 4 / length(data$cooksd_t), ]
```

#### Brown-Forsythe Test

```{r}
grp <- as.factor(c(rep("lower", floor(dim(data)[1] / 2)), 
                   rep("upper", ceiling(dim(data)[1] / 2))))
leveneTest(data[order(data$rent_transformed), "residuals"] ~ grp, center = median)
```

#### DFBETAS

```{r}
# calculate the DFBETAS
data.dfbetas_t <- as.data.frame(dfbetas(transformed_data.lm))
# add the observation numbers to dataframe for plotting purposes
data.dfbetas_t$obs <- 1:length(data$rent_transformed)

# plot the DFBETAS against the observation number
ggplot(data = data.dfbetas_t) + 
  geom_point(mapping = aes(x = obs, y = abs(rent_transformed))) +
  ylab("Absolute Value of DFBETAS for 1 / Average Rent") +
  xlab("Observation Number") +
  geom_hline(mapping = aes(yintercept = 2 / sqrt(length(obs))),
             color = "red", linetype = "dashed") +
  #scale_x_continuous(limits = c(0, 300)) +
  #scale_y_continuous(limits = c(0, 0.25)) +
  theme_bw() +
  theme(
    axis.title.x = element_text(size = sz - 5),
    axis.title.y = element_text(size = sz - 5),
    axis.text = element_text(size = sz - 5),
    aspect.ratio = 1,
    plot.title = element_blank()
  )

# print a list of potential influential points according to DFBETAS
rent.extreme.dfbetas_t <- data.dfbetas_t[abs(data.dfbetas_t$rent_transformed) > 
                                         2 / sqrt(length(data.dfbetas_t$obs)), ]
rent.extreme.dfbetas_t[order(rent.extreme.dfbetas_t$rent_transformed), ]
```

#### DFFITS

```{r}
# calculate the DFFITS
data.dffits_t <- data.frame("dffits" = dffits(transformed_data.lm))
# add the observation numbers to dataframe for plotting purposes
data.dffits_t$obs <- 1:length(data$crime_rate)

# plot the DFFITS against the observation number
ggplot(data = data.dffits_t) + 
  geom_point(mapping = aes(x = obs, y = abs(dffits))) +
  ylab("Absolute Value of DFFITS for Y") +
  xlab("Observation Number") +
  geom_hline(mapping = aes(yintercept = 2 * 
                             sqrt(length(transformed_data.lm$coefficients) / length(obs))),
             color = "red", linetype = "dashed") +
  theme_bw() +
  #scale_x_continuous(limits = c(0, 300)) +
  #scale_y_continuous(limits = c(0, 0.3)) +
  theme(
    axis.title.x = element_text(size = sz - 5),
    axis.title.y = element_text(size = sz - 5),
    axis.text = element_text(size = sz - 5),
    aspect.ratio = 1,
    plot.title = element_blank()
  )

# print a list of potential influential points according to DFFITS
data.dffits[abs(data.dffits_t$dffits) > 2 * 
             sqrt(length(transformed_data.lm$coefficients) / length(data.dffits_t$obs)), ]
```
### Assumption Checking for The Transformed Model

#### X vs Y is linear
X vs Y is linear, since the scatterplot does look roughly linear and does looks like a line can be easily fitted to the data. However that line may not have much of a slope. As well the Residuals vs Fitted values plot shows that the blue line is roughly horizontal with heavy curvature. 

#### The residuals are independent

The residuals seem not to be independent since crimes the recorded are only observed or reported to the FBI. Crime in general is not random but premeditated. However, average rent is more likely to be skewed by heavy outliers in cities depending on the area. Median rent prices might be a better indicator.

#### The residuals are normally distributed and centered at zero

The residuals are normally distributed. The box plot is pretty normal with the mean centered at zero with the whiskers having roughly equal lengths. The density and residual histogram for the transformed data looks normal. As well, the Q-Q plot for the transformed data seems to follow the diagonal dashed line very well meaning the residuals look normally distributed.

#### The residuals have equal/constant variance across all values of X

The residuals dohave equal/constant variance across all values of average rent. The residuals vs fitted values plot seems to show equal variance of average rent prices with slightly more variance on the right side . However, the brown-forsythe test for the transformed data gives us a non-significant p-value of 0.9866 which is greater than 0.05. Therefore we fail to reject the null hypothesis that the redisuals have equal variance across all values of average rent prices. 

#### The model describes all observations (i.e., there are no influential points)

The model seems to describe all observations, since the boxplot shows only about 4 outliers. The histogram shows possible a couple outliers. The Q-Q plot shows the same. However, the cooks distance shows many observation greater than 4/335 but they are all pretty close to this cut-off. There seems to be no influential points.

#### Additional predictor variables are not required.

Additional predictor variables may actually be required. Since it seems rent prices do not predict crime rates very well. Other predictor variables may be supply vs demand of housing in a city or State. Another predictor could be numbers of years since last renovations of housing. As well unemployment rates in a city could possibly predict crime rates.

#### A 95% confidence interval and Hypothesis test for the slope

```{r}
confint(transformed_data.lm, parm = "rent_transformed", level = 0.95)

summary(transformed_data.lm)

t.stat <- (transformed_data.lm$coefficients[2] - 0) / summary(transformed_data.lm)$coefficients[2, 2]
t.stat
pt(t.stat, df = nrow(data) - 2, lower.tail = FALSE) * 2
```
We are 95% confident that the average [Transformed y] increases between -.045 and .005 for every one increase in the sin of rent.
Because zero is concluding in this interval, we fail to reject the null hypothesis and cannot say whether or not there is a relationship between rent and crime rate. 

Looking at the hypothesis test for the slope we see that the it gave us a non significant p-value of 1.73 > .05. Therefore we fail to reject the null hypothesis that there is no a significant relationship between average rent and crime rates in a city in the United States. It is not appropriate to do a confidence or prediction interval on the average and individual crime rates of the transformed model. However below we have provided both to show that they do not contribute anything very valuable

#### A 95% Confidence and Prediciton interval for the average and individual values of Average Rent in a City

```{r}
predict(transformed_data.lm,
        newdata = data.frame(rent_transformed = 1200),
        interval = "confidence",
        level = 0.95)

predict(transformed_data.lm,
        newdata = data.frame(rent_transformed = 1200),
        interval = "prediction",
        level = 0.95)
```

Both the confidence interval the transformed average of crime rate where average rent is equal to 1200 and prediction interval for individual crime rates where average rent is equal to 1200  are roughly the same. The confidence interval is only a tiny bit narrower.

We are 95% confident that the transformed  average crime rate in a city is between -6.14 and 1.42 when the average rent is equal to 1200 dollars. As well we are 95% confident that the transformed individual crime rate for a city is between -6.14 and 1.42 when the average rent is equal to 1200.

#### MSE and RMSE of the Transformed Data
```{r}
anova <- aov(transformed_data.lm)
summary(anova)
mse <- summary(anova)[[1]][2, 2] / summary(anova)[[1]][2, 1] 
mse
sqrt(mse)
```

#### R-squared and Adjusted R-squared of the Transformed Data
```{r}
summary(transformed_data.lm)$r.squared
summary(transformed_data.lm)$adj.r.squared

```

The r square value tells us that .37% of crime is explained by average rent
The adjusted r-value represents  the proportion of variation in the crime rent explained by average rent, adjusted for the number of variables in the model. With .08% of the variability in the crime rate is  explained by average rent, after adjusting for the number of variables in the model, I would say this is not a good fit, and average rent is not very useful at predicting crime rates in a city.

#### F-statistic of the Transformed data
```{r}
f <- summary(transformed_data.lm)$fstatistic
f
pf(f[1], f[2], f[3], lower.tail = FALSE)
```

Our F-statistic gives us a non significant p-value which means we fail to reject the null hypothesis that the predictor of Crime rate has no linear relationship with average rent.


## Results and Conclusion

Although we hypothesized that rent would be able to predict crime rate, there seems to be no significant relationship between them. Because of this, we cannot use rent to predict crime rate, create accurate prediction intervals, or confidence intervals about a certain rent amount (although we did to show what we would do if it were significant). However, we were surprised that the residuals were pretty scattered as far as a sequence plot because we were concerned about the possibility of the time affecting the results. But this was not the case. 
I think to continue the analysis of rent and crime rate there are three main approaches. We can try to obtain a bigger dataset that includes many cities (more than just the metropolitan cities). If we collect data for more cities, we might be able to see a pattern between the rent prices and crime rate. Second, we could zoom in our focus onto certain areas of cities to get more robust data. Instead of just "Las Vegas", we would look at Summerlin, Henderson, North Las Vegas, Paradise, etc. Most of the cities we analyzed are large enough, they are broken up into different neighborhoods that could be analyzed. Finally, I believe looking into other variables might create a more interesting analysis. For this analysis we assumed rent prices were the only things to predict crime rate, but in reality we could look at quality of schools/education, average income, average number of people in a household, etc. 
This analysis, although with insignificant results, was very beneficial as we became more familiar with the datasets and applications of linear models in "real life" situations.
